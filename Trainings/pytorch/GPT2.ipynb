{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8cba2e8c034703b402c0faaa48aff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\guill\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00be0ab4c80440fab7e7e857037f99c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b013e4cc43414956b364dfa60d153786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f75a1822b994b58b2364c2294a5bd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfe14c2dd414a358a365e07d3b69989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c4aa6cca1e4149a11846b237784b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01761fd931548c69e042224869cee26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer  = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "seed_input = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(seed_input, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=40, \n",
    "    temperature=0.7, \n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "generated_text = ['the cat is on the mat']\n",
    "\n",
    "real_text = [[\n",
    "    'there is a cat on the mat',\n",
    "    'a cat is on the mat']\n",
    "    ]\n",
    "\n",
    "bleu = BLEUScore()\n",
    "bleu_metric = bleu(generated_text, real_text)\n",
    "print(\"BLEU Score: \", bleu_metric.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: ROUGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Stemmer and/or `rougeLsum` requires that `nltk` is installed. Use `pip install nltk`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello, how are you doing?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m real_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m rouge \u001b[38;5;241m=\u001b[39m \u001b[43mROUGEScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m rouge_score \u001b[38;5;241m=\u001b[39m rouge([generated_text], [[real_text]])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE Score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, rouge_score)\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchmetrics\\text\\rouge.py:117\u001b[0m, in \u001b[0;36mROUGEScore.__init__\u001b[1;34m(self, use_stemmer, normalizer, tokenizer, accumulate, rouge_keys, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_stemmer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeLsum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rouge_keys:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _NLTK_AVAILABLE:\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmer and/or `rougeLsum` requires that `nltk` is installed. Use `pip install nltk`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         )\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rouge_keys, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Stemmer and/or `rougeLsum` requires that `nltk` is installed. Use `pip install nltk`."
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import ROUGEScore\n",
    "\n",
    "generated_text = 'Hello, how are you doing?'\n",
    "real_text = 'Hello, how are you?'\n",
    "\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "rouge_score = rouge([generated_text], [[real_text]])\n",
    "print(\"ROUGE Score: \", rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"Once upon a time, there was a little girl who lived in a village near the forest.\"\n",
    "generated_text = \"Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\"\n",
    "\n",
    "# Initialize BLEU and ROUGE scorers\n",
    "bleu = BLEUScore()\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "# Calculate the BLEU and ROUGE scores\n",
    "bleu_score = bleu([generated_text], [[reference_text]])\n",
    "rouge_score = rouge([generated_text], [[reference_text]])\n",
    "\n",
    "# Print the BLEU and ROUGE scores\n",
    "print(\"BLEU Score:\", bleu_score.item())\n",
    "print(\"ROUGE Score:\", rouge_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/991.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/991.5 kB 325.1 kB/s eta 0:00:03\n",
      "   --- ----------------------------------- 92.2/991.5 kB 751.6 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 256.0/991.5 kB 1.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 440.3/991.5 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 522.2/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 604.2/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 634.9/991.5 kB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 665.6/991.5 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 665.6/991.5 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 686.1/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 696.3/991.5 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 716.8/991.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 716.8/991.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 727.0/991.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 747.5/991.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 768.0/991.5 kB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 944.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 944.6 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 798.7/991.5 kB 884.9 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 809.0/991.5 kB 838.1 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 829.4/991.5 kB 832.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 849.9/991.5 kB 802.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 776.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 880.6/991.5 kB 762.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/991.5 kB 741.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/991.5 kB 741.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 911.4/991.5 kB 720.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 911.4/991.5 kB 720.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 931.8/991.5 kB 685.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 942.1/991.5 kB 662.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 942.1/991.5 kB 662.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 962.6/991.5 kB 648.3 kB/s eta 0:00:01\n",
      "   -------------------------------------  972.8/991.5 kB 622.2 kB/s eta 0:00:01\n",
      "   -------------------------------------  972.8/991.5 kB 622.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 991.5/991.5 kB 603.9 kB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer,  T5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "input_prompt = \"translate English to Spanish: Hello, how are you?\"\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print('Generated text: ', generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
